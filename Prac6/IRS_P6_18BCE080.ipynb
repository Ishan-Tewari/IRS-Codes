{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRS Prac6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpSK-Wmza5u-"
      },
      "source": [
        "# Latent Semantic Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8lrlEI-3ke3"
      },
      "source": [
        "## Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C37YOBdkaMLC",
        "outputId": "c837b571-a82c-4cdd-eea6-b8db45309067"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0VhXJsDzEA"
      },
      "source": [
        "## Getting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw18prGI3onQ",
        "outputId": "899728e2-1007-4b22-cc67-eacf832bd24a"
      },
      "source": [
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
        "                          remove=('headers', 'footers', 'qoutes'))\n",
        "docs = dataset.data\n",
        "print(\"Number of documents : \", len(docs))\n",
        "print(\"Target names of dataset : \")\n",
        "print(dataset.target_names)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents :  11314\n",
            "Target names of dataset : \n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYB30erwEem2"
      },
      "source": [
        "## Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "zFFZrHs54COZ",
        "outputId": "8b82d0ff-3ae6-4f29-ae76-9a3bdc93d016"
      },
      "source": [
        "news_df = pd.DataFrame({'document':docs})\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
        "\n",
        "news_df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>clean_doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Well i'm not sure about the story nad it did s...</td>\n",
              "      <td>well sure about story seem biased what disagre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>James Hogan writes:\\n\\ntimmbake@mcl.ucsb.edu (...</td>\n",
              "      <td>james hogan writes timmbake ucsb bake timmons ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Although I realize that principle is not one o...</td>\n",
              "      <td>although realize that principle your strongest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
              "      <td>notwithstanding legitimate fuss about this pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Well, I will have to change the scoring on my ...</td>\n",
              "      <td>well will have change scoring playoff pool unf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11309</th>\n",
              "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
              "      <td>danny rubenstein israeli journalist will speak...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11310</th>\n",
              "      <td>ron.roth@rose.com (ron roth) writes:\\n\\n|JB&gt;  ...</td>\n",
              "      <td>roth rose roth writes romdas uclink berkeley e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11311</th>\n",
              "      <td>In article &lt;1qn6tqINNmnf@senator-bedfellow.MIT...</td>\n",
              "      <td>article tqinnmnf senator bedfellow athena char...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11312</th>\n",
              "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
              "      <td>used deskjet with orange micros grappler syste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11313</th>\n",
              "      <td>In article &lt;15APR199311534452@rosie.uh.edu&gt; st...</td>\n",
              "      <td>article rosie rosie schwam david writes articl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11314 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                document                                          clean_doc\n",
              "0      Well i'm not sure about the story nad it did s...  well sure about story seem biased what disagre...\n",
              "1      James Hogan writes:\\n\\ntimmbake@mcl.ucsb.edu (...  james hogan writes timmbake ucsb bake timmons ...\n",
              "2      Although I realize that principle is not one o...  although realize that principle your strongest...\n",
              "3      Notwithstanding all the legitimate fuss about ...  notwithstanding legitimate fuss about this pro...\n",
              "4      Well, I will have to change the scoring on my ...  well will have change scoring playoff pool unf...\n",
              "...                                                  ...                                                ...\n",
              "11309  Danny Rubenstein, an Israeli journalist, will ...  danny rubenstein israeli journalist will speak...\n",
              "11310  ron.roth@rose.com (ron roth) writes:\\n\\n|JB>  ...  roth rose roth writes romdas uclink berkeley e...\n",
              "11311  In article <1qn6tqINNmnf@senator-bedfellow.MIT...  article tqinnmnf senator bedfellow athena char...\n",
              "11312  I used HP DeskJet with Orange Micros Grappler ...  used deskjet with orange micros grappler syste...\n",
              "11313  In article <15APR199311534452@rosie.uh.edu> st...  article rosie rosie schwam david writes articl...\n",
              "\n",
              "[11314 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "jGGkJf4n4J-5",
        "outputId": "7a480e0a-8635-4521-a046-89bbe171869a"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "detokenized_doc = []\n",
        "for i in range(len(news_df)):\n",
        "    t = ' '.join(tokenized_doc[i])\n",
        "    detokenized_doc.append(t)\n",
        "\n",
        "news_df['clean_doc'] = detokenized_doc\n",
        "\n",
        "news_df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>clean_doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Well i'm not sure about the story nad it did s...</td>\n",
              "      <td>well sure story seem biased disagree statement...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>James Hogan writes:\\n\\ntimmbake@mcl.ucsb.edu (...</td>\n",
              "      <td>james hogan writes timmbake ucsb bake timmons ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Although I realize that principle is not one o...</td>\n",
              "      <td>although realize principle strongest points wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
              "      <td>notwithstanding legitimate fuss proposal much ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Well, I will have to change the scoring on my ...</td>\n",
              "      <td>well change scoring playoff pool unfortunately...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11309</th>\n",
              "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
              "      <td>danny rubenstein israeli journalist speaking t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11310</th>\n",
              "      <td>ron.roth@rose.com (ron roth) writes:\\n\\n|JB&gt;  ...</td>\n",
              "      <td>roth rose roth writes romdas uclink berkeley e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11311</th>\n",
              "      <td>In article &lt;1qn6tqINNmnf@senator-bedfellow.MIT...</td>\n",
              "      <td>article tqinnmnf senator bedfellow athena char...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11312</th>\n",
              "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
              "      <td>used deskjet orange micros grappler system upd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11313</th>\n",
              "      <td>In article &lt;15APR199311534452@rosie.uh.edu&gt; st...</td>\n",
              "      <td>article rosie rosie schwam david writes articl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11314 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                document                                          clean_doc\n",
              "0      Well i'm not sure about the story nad it did s...  well sure story seem biased disagree statement...\n",
              "1      James Hogan writes:\\n\\ntimmbake@mcl.ucsb.edu (...  james hogan writes timmbake ucsb bake timmons ...\n",
              "2      Although I realize that principle is not one o...  although realize principle strongest points wo...\n",
              "3      Notwithstanding all the legitimate fuss about ...  notwithstanding legitimate fuss proposal much ...\n",
              "4      Well, I will have to change the scoring on my ...  well change scoring playoff pool unfortunately...\n",
              "...                                                  ...                                                ...\n",
              "11309  Danny Rubenstein, an Israeli journalist, will ...  danny rubenstein israeli journalist speaking t...\n",
              "11310  ron.roth@rose.com (ron roth) writes:\\n\\n|JB>  ...  roth rose roth writes romdas uclink berkeley e...\n",
              "11311  In article <1qn6tqINNmnf@senator-bedfellow.MIT...  article tqinnmnf senator bedfellow athena char...\n",
              "11312  I used HP DeskJet with Orange Micros Grappler ...  used deskjet orange micros grappler system upd...\n",
              "11313  In article <15APR199311534452@rosie.uh.edu> st...  article rosie rosie schwam david writes articl...\n",
              "\n",
              "[11314 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLur2o-6Emf9"
      },
      "source": [
        "## Converting into TF-IDF Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAYVjWfX4Uu4",
        "outputId": "c9b01a0c-4773-48e6-82f9-163653664185"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', \n",
        "max_features= 2000,\n",
        "max_df = 0.5, \n",
        "smooth_idf=True)\n",
        "\n",
        "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
        "\n",
        "X"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<11314x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 440858 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUp1x_QsEtCE"
      },
      "source": [
        "## Creating and using a SVD Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wzI-bQR4Xly",
        "outputId": "d9400225-c6cd-41a8-8bad-c37ffd5f59d9"
      },
      "source": [
        "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
        "svd_model.fit(X)\n",
        "len(svd_model.components_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWDtzLH34aMX",
        "outputId": "0a87d697-1726-4dc1-8af6-6a41115f1996"
      },
      "source": [
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "for i, comp in enumerate(svd_model.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
        "    print(\"\\nTopic \"+str(i+1)+\": \\n\")\n",
        "    for t in sorted_terms:\n",
        "        print(t[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Topic 1: \n",
            "\n",
            "article\n",
            "like\n",
            "people\n",
            "know\n",
            "think\n",
            "good\n",
            "time\n",
            "\n",
            "Topic 2: \n",
            "\n",
            "windows\n",
            "thanks\n",
            "card\n",
            "drive\n",
            "file\n",
            "files\n",
            "mail\n",
            "\n",
            "Topic 3: \n",
            "\n",
            "game\n",
            "team\n",
            "games\n",
            "year\n",
            "season\n",
            "players\n",
            "play\n",
            "\n",
            "Topic 4: \n",
            "\n",
            "drive\n",
            "scsi\n",
            "drives\n",
            "hard\n",
            "controller\n",
            "disk\n",
            "floppy\n",
            "\n",
            "Topic 5: \n",
            "\n",
            "chip\n",
            "encryption\n",
            "clipper\n",
            "government\n",
            "keys\n",
            "space\n",
            "phone\n",
            "\n",
            "Topic 6: \n",
            "\n",
            "thanks\n",
            "mail\n",
            "advance\n",
            "know\n",
            "email\n",
            "looking\n",
            "address\n",
            "\n",
            "Topic 7: \n",
            "\n",
            "card\n",
            "video\n",
            "monitor\n",
            "drivers\n",
            "chip\n",
            "cards\n",
            "driver\n",
            "\n",
            "Topic 8: \n",
            "\n",
            "israel\n",
            "drive\n",
            "israeli\n",
            "game\n",
            "jews\n",
            "team\n",
            "government\n",
            "\n",
            "Topic 9: \n",
            "\n",
            "israel\n",
            "israeli\n",
            "article\n",
            "jews\n",
            "arab\n",
            "bike\n",
            "arabs\n",
            "\n",
            "Topic 10: \n",
            "\n",
            "space\n",
            "nasa\n",
            "sale\n",
            "card\n",
            "jesus\n",
            "earth\n",
            "shuttle\n",
            "\n",
            "Topic 11: \n",
            "\n",
            "window\n",
            "israel\n",
            "problem\n",
            "application\n",
            "motif\n",
            "display\n",
            "manager\n",
            "\n",
            "Topic 12: \n",
            "\n",
            "windows\n",
            "sale\n",
            "offer\n",
            "condition\n",
            "price\n",
            "shipping\n",
            "good\n",
            "\n",
            "Topic 13: \n",
            "\n",
            "article\n",
            "israel\n",
            "uiuc\n",
            "sale\n",
            "news\n",
            "jesus\n",
            "says\n",
            "\n",
            "Topic 14: \n",
            "\n",
            "window\n",
            "people\n",
            "article\n",
            "mail\n",
            "armenian\n",
            "armenians\n",
            "turkish\n",
            "\n",
            "Topic 15: \n",
            "\n",
            "windows\n",
            "window\n",
            "space\n",
            "thanks\n",
            "bike\n",
            "jesus\n",
            "nasa\n",
            "\n",
            "Topic 16: \n",
            "\n",
            "bike\n",
            "file\n",
            "armenian\n",
            "jesus\n",
            "armenians\n",
            "files\n",
            "card\n",
            "\n",
            "Topic 17: \n",
            "\n",
            "armenian\n",
            "armenians\n",
            "know\n",
            "turkish\n",
            "windows\n",
            "chip\n",
            "armenia\n",
            "\n",
            "Topic 18: \n",
            "\n",
            "know\n",
            "like\n",
            "monitor\n",
            "apple\n",
            "space\n",
            "window\n",
            "files\n",
            "\n",
            "Topic 19: \n",
            "\n",
            "scsi\n",
            "like\n",
            "list\n",
            "bike\n",
            "mail\n",
            "know\n",
            "people\n",
            "\n",
            "Topic 20: \n",
            "\n",
            "mail\n",
            "problem\n",
            "address\n",
            "mouse\n",
            "right\n",
            "game\n",
            "apple\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX47kEjk4dE6"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}