{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j4l0dtQbrFB-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flask import Flask,render_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WrEYOHqMrV6-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishan/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.22.2.post1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/ishan/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.22.2.post1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "X = pkl.load(open(\"X.pkl\",\"rb\"))\n",
    "tfidf = pkl.load(open(\"model.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Pok0q0Ir_pW",
    "outputId": "bc107b95-5ecf-414f-946f-fe8cc3985e76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ishan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ishan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nWOFCjE2rmzb"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  # removing punctuations\n",
    "    import string\n",
    "    punc = string.punctuation\n",
    "    for c in text:\n",
    "      if c in punc:\n",
    "        text = text.replace(c,\"\")\n",
    "    # print(text)\n",
    "\n",
    "    # tokenization\n",
    "    words = word_tokenize(text)\n",
    "    # print(\"After tokenization: {}\".format(words))\n",
    "\n",
    "    # stop words removal\n",
    "    clean_words=[]\n",
    "    for word in words:\n",
    "      if word not in stopwords:\n",
    "        clean_words.append(word)\n",
    "\n",
    "    words = clean_words\n",
    "    # print(\"After stop words removal: {}\".format(words))\n",
    "\n",
    "    # stemming\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "      stemmed_words.append(porter.stem(word))\n",
    "\n",
    "    words = stemmed_words\n",
    "    # print(\"After stemming: {}\".format(words))\n",
    "\n",
    "    # converting to lowercase\n",
    "    lowercase_words = []\n",
    "    for word in words:\n",
    "      lowercase_words.append(word.lower())\n",
    "\n",
    "    words = lowercase_words\n",
    "    # print(\"After converting to lowecase: {}\".format(words))\n",
    "\n",
    "    #adding to processed docs\n",
    "    processed_text = ' '.join(map(str, words))\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DweOpJcArv6s"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def searchQuery(k, query):\n",
    "  processed_query = preprocess(query)\n",
    "  query_vec = tfidf.transform([processed_query])\n",
    "\n",
    "  cosine_values = cosine_similarity(X,query_vec)\n",
    "  cosine_values = cosine_values.reshape((-1,))\n",
    "\n",
    "  results = []\n",
    "  cosine_sorted_index = cosine_values.argsort()[-k:][::-1]\n",
    "  # print(cosine_sorted_index)\n",
    "  for i in cosine_sorted_index:\n",
    "    results.append(('/content/p3_d{}.txt'.format(i+1),cosine_values[i]))\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jj1zwCLWr3dN",
    "outputId": "1c316f44-d902-4c8f-e8c1-3c691040ccf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/content/p3_d9.txt', 0.5397861890380344),\n",
       " ('/content/p3_d13.txt', 0.03436666319545569),\n",
       " ('/content/p3_d14.txt', 0.015905595269496132),\n",
       " ('/content/p3_d1.txt', 0.0038702100328060584),\n",
       " ('/content/p3_d15.txt', 0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"the simpsons american sitcom\"\n",
    "results = searchQuery(5,query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "7NN2YB-ttJNm"
   },
   "outputs": [],
   "source": [
    "# @app.route('/')\n",
    "def home():\n",
    "  return render_template('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "HvhVFiiNt_bh"
   },
   "outputs": [],
   "source": [
    "@app.route('/search',methods=['POST'])\n",
    "def search():\n",
    "  query = str(request.form.values())\n",
    "  results = searchQuery(10,query)\n",
    "\n",
    "  return render_template('index.html',doc_names=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "aPOdUjm6vOzI"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  # app.run(debug=True)\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFGozF1NvW0I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "app.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
